{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformers\n",
    "# Used to load pretrained tokenizers, models, and easy-to-use inference pipelines\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# TextBlob\n",
    "# Simple library for basic sentiment analysis and text polarity\n",
    "from textblob import TextBlob\n",
    "\n",
    "# CleanText\n",
    "# Used for cleaning raw text (removing emojis, special characters, etc.)\n",
    "from cleantext import clean\n",
    "\n",
    "# Pandas\n",
    "# Core library for working with DataFrames and CSV data\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK (Natural Language Toolkit)\n",
    "# Used for tokenization, stopwords, lemmatization, and sentiment analysis\n",
    "import nltk\n",
    "\n",
    "# spaCy\n",
    "# Advanced NLP library for POS tagging, dependency parsing, and NER\n",
    "import spacy\n",
    "\n",
    "# Stopwords from NLTK\n",
    "# Common words (the, is, and, etc.) removed during preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizer\n",
    "# Splits text into individual words (tokens)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lemmatizer\n",
    "# Converts words to their base form (e.g., \"running\" â†’ \"run\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Regular expressions\n",
    "# Used for pattern-based text cleaning\n",
    "import re\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('punkt')       # Tokenizer models\n",
    "nltk.download('stopwords')   # Stopword lists\n",
    "nltk.download('wordnet')     # Lemmatization dictionary\n",
    "\n",
    "# Load English stopwords into a set for fast lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NumPy\n",
    "# Numerical operations and array handling\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "# Backend used by Hugging Face models for inference\n",
    "import torch\n",
    "\n",
    "# VADER Sentiment Analyzer (NLTK)\n",
    "# Rule-based sentiment analysis optimized for short text\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Matplotlib\n",
    "# Used for plotting sentiment distributions, keyword frequency, etc.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af81c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV\n",
    "emails = pd.read_csv(\"emails.csv\", header=None)  # Use header=None if your CSV has no header\n",
    "emails.columns = [\"email_text\"]  # Name the single column\n",
    "\n",
    "# Add an ID column\n",
    "emails[\"id\"] = range(1, len(emails) + 1)\n",
    "\n",
    "# Reorder so 'id' is first\n",
    "emails = emails[[\"id\", \"email_text\"]]\n",
    "\n",
    "# Check the result\n",
    "print(emails.head())\n",
    "emails.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa63295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the email texts \n",
    "emails[\"text_clean\"] = emails[\"email_text\"].apply( lambda x: clean(x, no_emoji=True) ) \n",
    "\n",
    "emails[\"text_clean\"] = emails[\"text_clean\"].apply( lambda x: re.sub(r\"[^\\w\\s]\", \"\", x) )\n",
    "\n",
    "#  Tokenize text FIRST\n",
    "emails[\"tokens\"] = emails[\"text_clean\"].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords (NOW this works correctly)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "emails[\"tokens_no_stopwords\"] = emails[\"tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
    ")\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "emails[\"lemmatized_tokens\"] = emails[\"tokens_no_stopwords\"].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
    ")\n",
    "\n",
    "#clean tokens to keep only alphanumeric\n",
    "emails[\"clean_tokens\"] = emails[\"lemmatized_tokens\"].apply(\n",
    "    lambda tokens: [token for token in tokens if token.isalnum()]\n",
    ")\n",
    "\n",
    "emails.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the POS tagger data\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to process tokens and get POS tags\n",
    "def spacy_pos(tokens):\n",
    "    # Join tokens into a single string (spaCy works on text)\n",
    "    text = \" \".join(tokens)\n",
    "    doc = nlp(text)\n",
    "    # Return a list of (token, POS) tuples\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Apply to your DataFrame\n",
    "emails['pos_tags'] = emails['clean_tokens'].apply(spacy_pos)\n",
    "\n",
    "# Preview\n",
    "print(emails[['email_text', 'clean_tokens', 'pos_tags']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24609c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails[emails['id'] == 2]['pos_tags'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Load phishing-related keywords from file\n",
    "# ---------------------------------------------\n",
    "# Each line in phishing_keywords.txt represents\n",
    "# one keyword commonly found in phishing emails\n",
    "with open(\"phishing_keywords.txt\", \"r\") as f:\n",
    "    # Strip whitespace, convert to lowercase,\n",
    "    # and ignore empty lines\n",
    "    keywords = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Function to flag presence of phishing keywords\n",
    "# ---------------------------------------------\n",
    "def keyword_flags(tokens):\n",
    "    \"\"\"\n",
    "    Given a list of cleaned tokens from an email,\n",
    "    return a dictionary indicating whether each\n",
    "    phishing keyword is present (1) or not (0).\n",
    "    \"\"\"\n",
    "    return {f\"{k}\": int(k in tokens) for k in keywords}\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Apply keyword flagging to each email\n",
    "# ---------------------------------------------\n",
    "# Expand the dictionary of keyword flags into\n",
    "# individual dataframe columns\n",
    "keyword_df = emails[\"clean_tokens\"].apply(keyword_flags).apply(pd.Series)\n",
    "\n",
    "# Merge keyword feature columns back into main dataset\n",
    "emails = pd.concat([emails, keyword_df], axis=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Visualize phishing keyword frequency\n",
    "# ---------------------------------------------\n",
    "# Count how many emails contain each keyword\n",
    "keyword_counts = keyword_df.sum().sort_values()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(10, 20))\n",
    "keyword_counts.plot.barh()\n",
    "\n",
    "# Chart labeling\n",
    "plt.title(\"Phishing Keyword Frequency\")\n",
    "plt.xlabel(\"Number of Emails Containing Keyword\")\n",
    "plt.ylabel(\"Keyword\")\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Count how often each token-POS pair appears\n",
    "# --------------------------------\n",
    "pos_df_counts = (\n",
    "    emails.explode(\"pos_tags\")\n",
    "    .assign(token=lambda df: df['pos_tags'].str[0], pos_tag=lambda df: df['pos_tags'].str[1])\n",
    "    .groupby(['token', 'pos_tag'])\n",
    "    .size()\n",
    "    .reset_index(name=\"counts\")\n",
    "    .sort_values(\"counts\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show the 10 most common token + POS combinations\n",
    "pos_df_counts.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Count how many unique tokens belong to each POS tag\n",
    "# --------------------------------\n",
    "\n",
    "unique_tokens_per_pos = (\n",
    "    pos_df_counts.groupby(\"pos_tag\")[\"token\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"unique_token_count\")\n",
    "    .sort_values(\"unique_token_count\", ascending=False)\n",
    ")\n",
    "unique_tokens_per_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# VADER imports for rule-based sentiment analysis\n",
    "# ---------------------------------------------\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "emails[\"vader_sentiment_score\"] = emails[\"text_clean\"].apply(\n",
    "    lambda r: analyzer.polarity_scores(r)[\"compound\"]\n",
    ")\n",
    "\n",
    "emails.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12977d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment score ranges for classification\n",
    "# VADER compound scores range from -1 (very negative) to +1 (very positive)\n",
    "bins = [-1, -0.1, 0.1, 1]\n",
    "\n",
    "# Human-readable sentiment labels for each range\n",
    "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Convert numeric VADER sentiment scores into categorical labels\n",
    "# pd.cut assigns each score to a bin based on the ranges above\n",
    "emails[\"vader_sentiment_label\"] = pd.cut(\n",
    "    emails[\"vader_sentiment_score\"],\n",
    "    bins=bins,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "# Count how many emails fall into each sentiment category\n",
    "# and plot the distribution as a bar chart\n",
    "emails[\"vader_sentiment_label\"].value_counts().plot.bar()\n",
    "\n",
    "# Add a title and axis labels to the plot\n",
    "plt.title(\"VADER Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Hugging Face pipeline utility for easy model inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pretrained BERT model fine-tuned specifically for phishing detection\n",
    "# \"text-classification\" tells Transformers we want a classifier\n",
    "# truncation=True ensures long emails are safely trimmed to the model's max length\n",
    "phish_detection = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"ealvaradob/bert-finetuned-phishing\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Loop through each cleaned email text in the dataset\n",
    "for e in emails[\"text_clean\"]:\n",
    "    \n",
    "    # Run the phishing detection model on the email text\n",
    "    result = phish_detection(e)\n",
    "    \n",
    "    # Print the original email text\n",
    "    print(f\"Text: {e}\")\n",
    "    \n",
    "    # Print the model's prediction label and confidence score\n",
    "    print(f\"Prediction: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
